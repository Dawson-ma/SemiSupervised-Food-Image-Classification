{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised Food Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, SequentialSampler, Dataset\n",
    "from torchvision.datasets import DatasetFolder\n",
    "\n",
    "from warmup_scheduler import GradualWarmupScheduler # https://github.com/ildoonet/pytorch-gradual-warmup-lr\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown '1vufDjKxj4IwRni11uxjM0CSim5WehscA' --output food-11.zip\n",
    "!unzip -q food-11.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pseudo_dataset(Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.target = target\n",
    "        print(len(self.data))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        lbl = self.target[index]\n",
    "        \n",
    "        return img, lbl\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_size = 200\n",
    "train_ori = transforms.Compose([\n",
    "    # Resize the image into a fixed shape\n",
    "    transforms.Resize((inputs_size, inputs_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    # Resize the image into a fixed shape\n",
    "    transforms.Resize((inputs_size, inputs_size)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomAffine(45, translate=(0.1, 0.1), shear=[25, 25]),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5, fill=0),\n",
    "    transforms.ColorJitter(brightness=[0.9, 1.3], contrast=0.3, saturation=0.1, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.2)),\n",
    "])\n",
    "\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.Resize((inputs_size, inputs_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set_tra = DatasetFolder(\"food-11/training/labeled\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=train_tfm)\n",
    "train_set_ori = DatasetFolder(\"food-11/training/labeled\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=train_ori)\n",
    "train_set = ConcatDataset([train_set_tra, train_set_ori])\n",
    "valid_set = DatasetFolder(\"food-11/validation\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=test_tfm)\n",
    "unlabeled_set = DatasetFolder(\"food-11/training/unlabeled\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=train_tfm)\n",
    "test_set = DatasetFolder(\"food-11/testing\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=test_tfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Psudo Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pseudo_labels(dataset, model, threshold=0.7):\n",
    "    '''\n",
    "    This functions generates pseudo-labels of a dataset using given model.\n",
    "    It returns an instance of DatasetFolder containing images whose prediction confidences exceed a given threshold.\n",
    "    '''\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Make sure the model is in eval mode.\n",
    "    model.eval()\n",
    "    # Define softmax function.\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4,\n",
    "        pin_memory=True, drop_last=True)\n",
    "\n",
    "    data = []\n",
    "    target = []\n",
    "    pseudo_label = []\n",
    "    # Iterate over the dataset by batches.\n",
    "    for batch in tqdm(dataloader):\n",
    "    # for i, batch in enumerate(dataLoader):\n",
    "        img, _ = batch\n",
    "\n",
    "        # Forward the data\n",
    "        # Using torch.no_grad() accelerates the forward process.\n",
    "        with torch.no_grad():\n",
    "            logits = model(img.to(device))\n",
    "\n",
    "        # Obtain the probability distributions by applying softmax on logits.\n",
    "        probs = softmax(logits)\n",
    "\n",
    "        # Filter the data and construct a new dataset.\n",
    "        probs = [torch.argmax(x) if torch.max(x) > threshold else -1 for x in probs]\n",
    "        pseudo_label = np.array(probs)\n",
    "        pseudo_indice = np.where(pseudo_label != -1)\n",
    "        pseudo_indice = list(pseudo_indice[0])\n",
    "        for i in pseudo_indice:\n",
    "            data.append(img[i].tolist())\n",
    "            target.append(pseudo_label[i].item())\n",
    "\n",
    "    pseudo_set = pseudo_dataset(data, target)\n",
    "\n",
    "    print()\n",
    "\n",
    "    # # Turn off the eval mode.\n",
    "    model.train()\n",
    "    return pseudo_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"cuda\" only when GPUs are available.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize a model, and put it on the device specified.\n",
    "model = torchvision.models.resnet18(pretrained=False).to(device)\n",
    "model.device = device\n",
    "model.load_state_dict(torch.load('model.ckpt'))\n",
    "save_path = 'model_pseudo.ckpt'\n",
    "\n",
    "n_epochs = 500\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0008, weight_decay=2e-5)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=0.1*n_epochs, after_scheduler=scheduler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-Supervised Learning Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_semi = True\n",
    "\n",
    "ratio_top = 0.7\n",
    "ratio_bottom = 0.2\n",
    "\n",
    "top_threshold = 0.99\n",
    "bottom_threshold = 0.995\n",
    "\n",
    "tmp_pseudoset = Subset(unlabeled_set, [])\n",
    "best_acc = 0\n",
    "valid_acc = 0\n",
    "train_acc = 0\n",
    "last_acc = 0\n",
    "\n",
    "switch = False\n",
    "check = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "while epoch < n_epochs:\n",
    "    # In each epoch, relabel the unlabeled dataset for semi-supervised learning.\n",
    "    # Then combine the labeled dataset and pseudo-labeled dataset for the training.\n",
    "\n",
    "    threshold = bottom_threshold\n",
    "\n",
    "    if best_acc>0.7:\n",
    "        \n",
    "        # Obtain pseudo-labels for unlabeled data using trained model.\n",
    "        if valid_acc >= last_acc:\n",
    "            last_acc = valid_acc\n",
    "            pseudo_set = get_pseudo_labels(unlabeled_set, model, threshold)\n",
    "            tmp_pseudoset = pseudo_set\n",
    "\n",
    "        # Construct a new dataset and a data loader for training.\n",
    "        # This is used in semi-supervised learning only.\n",
    "        concat_dataset = ConcatDataset([train_set, tmp_pseudoset])\n",
    "\n",
    "        train_loader = DataLoader(concat_dataset, batch_size=batch_size, shuffle=True\n",
    "                        , num_workers=8, pin_memory=True, drop_last=True)\n",
    "\n",
    "    # ---------- Training ----------\n",
    "    # Make sure the model is in train mode before training.\n",
    "    model.train()\n",
    "    scheduler_warmup.step(epoch)\n",
    "\n",
    "    # These are used to record information in training.\n",
    "    train_loss = []\n",
    "    train_accs = []\n",
    "\n",
    "    # Iterate the training set by batches.\n",
    "    for batch in tqdm(train_loader):\n",
    "        # A batch consists of image data and corresponding labels.\n",
    "        imgs, labels = batch\n",
    "\n",
    "        # Forward the data.\n",
    "        logits = model(imgs.to(device))\n",
    "\n",
    "        # Calculate the cross-entropy loss.\n",
    "        loss = criterion(logits, labels.to(device))\n",
    "\n",
    "        # Gradients stored in the parameters in the previous step should be cleared out first.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the gradients for parameters.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradient norms for stable training.\n",
    "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "\n",
    "        # Update the parameters with computed gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute the accuracy for current batch.\n",
    "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        train_loss.append(loss.item())\n",
    "        train_accs.append(acc)\n",
    "\n",
    "    # The average loss and accuracy of the training set is the average of the recorded values.\n",
    "    train_loss = sum(train_loss) / len(train_loss)\n",
    "    train_acc = sum(train_accs) / len(train_accs)\n",
    "\n",
    "    # Print the information.\n",
    "    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
    "    print(f'lr = {get_lr(optimizer):3.6f}, threshold: {threshold:.4f}')\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
    "    model.eval()\n",
    "\n",
    "    # These are used to record information in validation.\n",
    "    valid_loss = []\n",
    "    valid_accs = []\n",
    "\n",
    "    # Iterate the validation set by batches.\n",
    "    for batch in tqdm(valid_loader):\n",
    "        # A batch consists of image data and corresponding labels.\n",
    "        imgs, labels = batch\n",
    "\n",
    "        # Using torch.no_grad() accelerates the forward process.\n",
    "        with torch.no_grad():\n",
    "          logits = model(imgs.to(device))\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(logits, labels.to(device))\n",
    "\n",
    "        # Compute the accuracy for current batch.\n",
    "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        valid_loss.append(loss.item())\n",
    "        valid_accs.append(acc)\n",
    "\n",
    "    # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
    "    valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "    valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "    if valid_acc > best_acc:\n",
    "        best_acc = valid_acc\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f'saving model with acc {best_acc:.5f}')\n",
    "\n",
    "    # Print the information.\n",
    "    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
    "    print()\n",
    "    epoch+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model is in eval mode.\n",
    "# Some modules like Dropout or BatchNorm affect if the model is in training mode.\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store the predictions.\n",
    "predictions = []\n",
    "\n",
    "# Iterate the testing set by batches.\n",
    "for batch in tqdm(test_loader):\n",
    "    # A batch consists of image data and corresponding labels.\n",
    "    # But here the variable \"labels\" is useless since we do not have the ground-truth.\n",
    "    # If printing out the labels, it is always 0.\n",
    "    # This is because the wrapper (DatasetFolder) returns images and labels for each batch,\n",
    "    # so we have to create fake labels to make it work normally.\n",
    "    imgs, labels = batch\n",
    "\n",
    "    # We don't need gradient in testing, and we don't even have labels to compute loss.\n",
    "    # Using torch.no_grad() accelerates the forward process.\n",
    "    with torch.no_grad():\n",
    "        logits = model(imgs.to(device))\n",
    "\n",
    "    # Take the class with greatest logit as prediction and record it.\n",
    "    predictions.extend(logits.argmax(dim=-1).cpu().numpy().tolist())\n",
    "\n",
    "# Save predictions into the file.\n",
    "with open(\"predict.csv\", \"w\") as f:\n",
    "\n",
    "    # The first row must be \"Id, Category\"\n",
    "    f.write(\"Id,Category\\n\")\n",
    "\n",
    "    # For the rest of the rows, each image id corresponds to a predicted class.\n",
    "    for i, pred in  enumerate(predictions):\n",
    "         f.write(f\"{i},{pred}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
